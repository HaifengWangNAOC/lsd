#!/usr/bin/env python
#
# Example: lsd-check object-table
#

import os
import lsd
import numpy as np
from collections import OrderedDict, defaultdict
from lsd.tui import *
from lsd.fetcher import Fetcher
from lsd.utils import mkdir_p
import fnmatch, glob
import sys

################
#
# Actual work
#

class InvalidArgumentError(Exception):
	pass

def do_create_table(args):
	# Check argument validity
	if args.schema_module is None and args.schema is None and len(args.column_def) == 0:
		raise InvalidArgumentError("Table schema not specified. Please specify the schema via column_def arguments, or --schema-module parameter")

	if args.schema is not None:
		# Load the schema from YAML file
		import lsd.yaml_ex as yaml
		schema = yaml.safe_load(file(args.schema))
	elif args.schema_module is not None:
		# Load the schema from module
		import importlib
		schema = importlib.import_module(args.schema_module).schema
	else:
		# Load the schema from the command line
		schema = OrderedDict()

		# Set up compression
		if args.comp != 'none':
			filters = { 'complib': args.comp, 'complevel': args.comp_level }

		# Construct the list of columns
		columns = OrderedDict()
		for column_def in args.column_def:
			(name, type) = (column_def.split(':') + ['f8'])[:2]	# Default datatype is f8
			try:
				_ = np.dtype(type)
			except TypeError:
				raise TypeError("Data type '%s' not understood for column %s" % (type, name))

			if name[0] == '_':
				raise Exception("Column names beginning with _ are not permitted.")

			columns[name] = { 'type': type, 'cgroup': 'default' }

		# Group the columns into column groups
		cgroups = OrderedDict()
		for cgroup_def in args.group:
			cgroup_name, cols = cgroup_def.split(':')	# Split colum group name from the list of columns
			cols = cols.split(',')				# Split the list of columns

			# Define the column group
			cgroups[cgroup_name] = { 'columns': [] }
			ccols = cgroups[cgroup_name]['columns']
			for colname in cols:
				ccols.append((colname, columns[colname]['type']))
				columns[colname]['cgroup'] = cgroup_name

		# Collect any remaining ungrouped columns and pack them into one named 'default'
		default_cgroup = [ (name, default['type']) for (name, default) in columns.iteritems() if default['cgroup'] == 'default' ]
		if len(default_cgroup):
			cgroups['default'] = { 'columns': default_cgroup }

		# Add primary key information
		prim_cgroup_name, prim_cgroup = next(cgroups.iteritems())
		if getattr(args, 'primary_key', None) is None:
			raise Exception("Must specify the primary key")
			columns['_id'] = { 'type': 'u8', 'cgroup': prim_cgroup_name }
			prim_cgroup['columns'].insert(0, ('_id', 'u8'))
			args.primary_key = '_id'
		assert prim_cgroup_name == columns[args.primary_key]['cgroup'] # Must be in the primary cgroup
		assert np.dtype(columns[args.primary_key]['type']) == np.uint64 # Primary key must be an unsigned 64-bit integer
		prim_cgroup['primary_key'] = args.primary_key

		# Add temporal key information
		if getattr(args, 'temporal_key', None) is not None:
			assert prim_cgroup_name == columns[args.temporal_key]['cgroup'] # Must be in the primary cgroup
			cgroups[ columns[args.temporal_key]['cgroup'] ]['temporal_key'] = args.temporal_key

		# Add exposure key information
		if getattr(args, 'exposure_key', None) is not None:
			cgroups[ columns[args.exposure_key]['cgroup'] ]['exposure_key'] = args.exposure_key

		# Add spatial keys information
		commit_hooks = []
		if getattr(args, 'spatial_keys', None) is not None:
			lon, lat = args.spatial_keys.split(',')
			assert prim_cgroup_name == columns[lon]['cgroup'] # Must be in the primary cgroup
			assert prim_cgroup_name == columns[lat]['cgroup'] # Must be in the primary cgroup
			prim_cgroup['spatial_keys'] = [lon, lat]

			if not args.no_neighbor_cache:
				commit_hooks = lsd.table.Table._default_commit_hooks
		else:
			raise Exception("Spatial keys must be specified")

		# Put it all together
		schema["filters"] = filters
		schema["schema"] = cgroups
		schema["commit_hooks"] = commit_hooks

	# Drop if exists
	if args.drop_existing and os.path.isdir(os.path.join(args.db, args.table)):
		drop_table(args.db, args.table)

	# Now create the table
	db = lsd.DB(args.db)
	with db.transaction():
		db.create_table(args.table, schema)

	print "Table '%s' created." % args.table

def do_desc_table(args):
	db = lsd.DB(args.db)
	table = db.table(args.table)
	print table

def do_remote_publish_table(args):
	db = lsd.DB(args.db)

	if args.update:
		if len(args.table):
			raise Exception('Tables cannot be gvien explicitly when --update is active')

		# Load the already published tables
		args.table = [ s.strip() for s in open(os.path.join(args.db, '.listing')).xreadlines() if os.path.isdir(os.path.join(args.db, s.strip())) and db.table_exists(s.strip()) ]
	else:
		if not len(args.table):
			raise Exception('At least one table must be given.')

	for table in args.table:
		t = db.table(table)
		snaps = t.list_snapshots()
		if len(snaps) == 1 and snaps[0] == 0:
			mkdir_p('%s/snapshots' % t.path)

		# Publish the snapshots within the table
		# NOTE: ignore_missing=True is set to support <v0.5.0 tables, those that have the implicit snapid=0
		publish_objects(os.path.join(t.path, 'snapshots'), snaps, overwrite=True, ignore_missing=True)
		print 'Table %s: %s snapshots published.' % (table, len(snaps))

	# Publish the tables
	publish_objects(args.db, args.table)
	print "Published tables %s." % (', '.join(args.table))

def do_remote_unpublish_table(args):
	unpublish_objects(args.db, args.table)
	for table in args.table:
		print "Unpublished %s" % table

def do_remote_unpublish_join(args):
	# Load existing
	exjoins = set(s.strip() for s in open(os.path.join(args.db, '.listing')).xreadlines() if s.strip().endswith('.join'))

	joins = set()
	for pat in args.join:
		# Interpret as pattern and add all existing joins that match it.
		joins.update(join for join in exjoins if fnmatch.fnmatch(join, pat))

		# Also allow patterns for just the join name, w/o the leading dot and trailing .join
		pat = '.%s.join' % pat
		joins.update(join for join in exjoins if fnmatch.fnmatch(join, pat))

	unpublish_objects(args.db, joins)
	for join in joins:
		print "Unpublished %s" % join[1:-5]

def do_remote_list(args):
	fetcher = Fetcher(args.remote)

	lst = defaultdict(list)
	for obj in fetcher.listdir(''):
		if obj.endswith('.join'):
			typ = 'JOIN'
			obj = obj[1:-5]
		else:
			typ = 'TABLE'

		lst[typ].append(obj)

	for typ, l in lst.iteritems():
		for obj in l:
			print "%10s %s" % (typ, obj)

def _safe_write_replace(fn, lst):
	# Save (save-to-temp-and-rename idiom)
	tmp = fn + '.part'
	with open(tmp, 'w') as fp:
		for obj in lst:
			fp.write('%s\n' % obj)
	os.rename(tmp, fn)

def unpublish_objects(dir, objects):
	""" Unpublish the given list of objects """
	lfn = os.path.join(dir, '.listing')

	exobjs = set(s.strip() for s in open(lfn).xreadlines())
	exobjs -= set(objects)

	_safe_write_replace(lfn, exobjs)

def publish_objects(dir, objects, overwrite=False, ignore_missing=False):
	""" Publish the given list of objects, that are assumed to
	    exist in directory dir. The list will be merged with
	    existing objects in dir/.listing.
	"""
	lfn = os.path.join(dir, '.listing')

	# Sanity check the objects list
	if not ignore_missing:
		for obj in objects:
			path = os.path.join(dir, str(obj))
			if not os.path.exists(path):
				raise Exception("The object '%s', that you're trying to publish, does not exist." % path)

	# Load contents of .listing
	if not overwrite and os.path.isfile(lfn):
		exobjs = set(s.strip() for s in open(lfn).xreadlines())
	else:
		exobjs = set()

	# Merge the two lists
	exobjs |= set(objects)

	_safe_write_replace(lfn, exobjs)

def do_remote_publish_join(args):
	joins = []
	for join in args.join:

		jpath = os.path.join(args.db, '.%s.join' % join)
		if os.path.isfile(jpath):	# Is it a join definition name, xxxx:yyyy
			joins.append(jpath)
		else:
			jpath = os.path.join(args.db, join)
			if join[-5:] == '.join' and os.path.isfile(jpath):	# Is it a .join file name relative to the database path?
				joins.append(jpath)
			else:
				dbpath = os.path.abspath(args.db)
				gg = [ g for g in glob.iglob(join) if os.path.abspath(os.path.dirname(g)) == dbpath and g[-5:] == '.join']	# Is it a pattern?
				joins += gg

	if joins:
		joins = set(os.path.basename(jpath) for jpath in joins)
		publish_objects(args.db, joins)

		for join in joins:
			print 'Join %s published.' % join[1:-5]
	else:
		print 'No join definition files found matching your specifications'

def do_remote_follow_table(args):
	db = lsd.DB(args.db)

	force = args.force
	for table in args.table:
		table_dir = os.path.join(args.db, table)
		if os.path.exists(table_dir):
			if args.drop_existing:
				drop_table(args.db, table)
			elif not args.force:
				raise Exception("Table %s must not already exist" % table)
		else:
			os.makedirs(table_dir)

		with open(os.path.join(table_dir, '.remote'), 'w') as fp:
			fp.write('%s/%s\n' % (args.remote, table))

		try:
			_ = db.table(table)
		except:
			# Something has gone wrong; clean up
			if not args.force:
				drop_table(args.db, table, quiet=True)
			raise

		print "Table '%s' follows remote table '%s/%s'" % (table, args.remote, table)

def do_remote_unfollow_table(args):
	for table in args.table:
		dot_remote = os.path.join(args.db, table, '.remote')
		if os.path.isfile(dot_remote):
			remote = open(dot_remote).read().strip()
			os.unlink(dot_remote)
			print "Table '%s' stopped following '%s'." % (table, remote)

def do_remote_fetch_join(args):
	db = lsd.DB(args.db)

	fetcher = Fetcher(args.remote)

	pat = '.%s.join' % args.pattern
	for fn in fetcher.listdir(''):
		if not fnmatch.fnmatch(fn, pat):
			continue
		fetcher.fetch_to_file(fn, os.path.join(args.db, fn))
		print "Fetched %s" % fn

def _fetch_table_mapper(qresult, db, table):
	uri_prefix = 'lsd:%s:' % table
	for rows in qresult:
		if len(rows) == 0:
			continue

		# Try to autodetect LSD URIs and fetch them
		for col in rows.as_columns():
			if type(col.flat[0]) != str:
				continue

			for s in np.unique(col):
				if s.startswith(uri_prefix):
					# This will download the file pointed to by the URI if it
					# hasn't been downloaded already
					with db.open_uri(s) as fp:
						pass

		yield len(rows)

def do_remote_fetch_table(args):
	db = lsd.DB(args.db)
	
	for table in args.table:
		print >>sys.stderr, "Fetching %s:" % table,
		q = db.query("SELECT * FROM '%s'" % table)
		total = sum(q.execute([(_fetch_table_mapper, db, table)]))
		print >>sys.stderr, "    %s rows fetched." % total
		assert total == db.table(table).nrows()

def get_snapshots_info(dbpath, table_name):
	db = lsd.DB(dbpath)
	# See if the database has a transaction opened
	tfile = '%s/.__transaction' % (dbpath)
	try:
		with open(tfile) as fp:
			cur_snapid = fp.read().strip()
	except:
		cur_snapid = None

	snaps = {}

	try:
		table = db.table(table_name)
		import glob
		for snap_path in glob.iglob('%s/snapshots/*' % table.path):
			snapid = snap_path.split('/')[-1]
			committed = os.path.isfile(os.path.join(snap_path, '.committed'))

			if not committed:
				status = "FAILED" if cur_snapid != snapid else "OPEN"
			else:
				status = "COMMITTED"

			snaps[snapid] = { 'status': status, 'path': snap_path }
	except:
		import sys
		print >>sys.stderr, "Problem reading table %s" % table_name

	return snaps

def do_desc_snapshots(args):
	if not args.table:
		# Enumerate all tables
		args.table = [ table for table in os.listdir(args.db) if os.path.isdir('%s/%s' % (args.db, table)) ]

	for table in args.table:
		snaps = get_snapshots_info(args.db, table)

		print "%33s" % table
		print "%20s %12s" % ("Snapshot ID", "State")
		print "-"*33

		for snapid in sorted(snaps.keys()):
			status = snaps[snapid]['status']

			print "%20s %11s" % (snapid, status)
		print ''

def do_vacuum_table(args):
	from pipes import quote
	from shutil import rmtree
	import sys

	if not args.table:
		# Enumerate all tables
		args.table = [ table for table in os.listdir(args.db) if os.path.isdir('%s/%s' % (args.db, table)) ]

	for table in args.table:
		snaps = { snapid:v for snapid, v in get_snapshots_info(args.db, table).iteritems() if v['status'] == 'FAILED' }

		for snapid in sorted(snaps.keys()):
			status = snaps[snapid]['status']
			path = snaps[snapid]['path']

			print "Deleting %s:%s ..." % (table, snapid),
			sys.stdout.flush()
			
			if not args.dry_run:
				os.system('chmod -R +w %s' % quote(path))
				rmtree(path)

			print "done."
	print "Vacuuming completed."

def drop_table(dbpath, table, quiet=False):
	path = os.path.join(dbpath, table)
	if not os.path.isdir(path):
		raise Exception("Table %s does not exist at %s" % (table, path))

	# Recursive chmod
	from pipes import quote
	os.system('chmod -R +w %s' % quote(path))

	from shutil import rmtree
	rmtree(path)

	if not quiet:
		print "Table '%s' dropped." % table

def do_drop_table(args):
	for table in args.table:
		drop_table(args.db, table)

################

import argparse
parser = argparse.ArgumentParser(description='Administer an LSD databases',
	formatter_class=argparse.RawDescriptionHelpFormatter,
	epilog="""Examples:
	lsd-admin create table ....
.
""")
parser.add_argument('--db', default=os.getenv('LSD_DB', '').split(':')[0], type=str, help='Path to LSD database')
subparsers = parser.add_subparsers()

# CREATE
parser_create = subparsers.add_parser('create', help='Create objects in the database')
subparsers2 = parser_create.add_subparsers()

# CREATE TABLE
parser_create_table = subparsers2.add_parser('table', help='Create a table')
parser_create_table.add_argument('table', type=str, help='Name of the table to create')
parser_create_table.add_argument('column_def', type=str, help="A column definition, in the form of NAME:TYPE, where TYPE is a string that will be passed to np.dtype", nargs='*')
parser_create_table.add_argument('--primary-key', type=str, help='Primary key')
parser_create_table.add_argument('--spatial-keys', type=str, help='Spatial keys')
parser_create_table.add_argument('--temporal-key', type=str, help='Temporal key')
parser_create_table.add_argument('--exposure-key', type=str, help='Exposure key')
parser_create_table.add_argument('--no-neighbor-cache', type=bool, default=False, help="No neighbor caches will be built for this table (note: use only if you _really_ know what you're doing")
parser_create_table.add_argument('--schema-module', type=str, help='The module defining the table schema, in top-level variable named "schema"')
parser_create_table.add_argument('--schema', type=str, help='A file containing the table schema in YAML format')
parser_create_table.add_argument('--comp', type=str, default='blosc', help='Compression type', choices=['blosc', 'zlib', 'none'])
parser_create_table.add_argument('--comp-level', type=int, default=5, help='Compression level. Higher levels result in better compression ratios, at the expense of increased CPU usage')
parser_create_table.add_argument('--group', type=str, default=[], action='append', help='cgroup:col1,col2,col3,... The column group name, followed by the list of columns to be located in this column group')
parser_create_table.add_argument('--drop-existing', help='Drop the table if it already exists', default=False, action='store_true')
parser_create_table.set_defaults(func=do_create_table)

# DESC
parser_desc = subparsers.add_parser('desc', help='Show info about tables and other objects in the database')
subparsers2 = parser_desc.add_subparsers()

# DESC TABLE
parser_desc_table = subparsers2.add_parser('table', help='Show a table schema')
parser_desc_table.add_argument('table', type=str, help='Name of the table')
parser_desc_table.set_defaults(func=do_desc_table)

# DESC SNAPSHOTS
parser_desc_snapshots = subparsers2.add_parser('snapshots', help="Show information on a table's snapshots")
parser_desc_snapshots.add_argument('table', type=str, help='Zero or more tables to describe. If left unspecified, all tables will be described', nargs='*')
parser_desc_snapshots.set_defaults(func=do_desc_snapshots)

# DROP
parser_drop = subparsers.add_parser('drop', help='Delete database tables and objects')
subparsers2 = parser_drop.add_subparsers()

# DROP TABLE
parser_drop_table = subparsers2.add_parser('table', help='Delete a table')
parser_drop_table.add_argument('table', type=str, help='One or more tables to delete', nargs='+')
parser_drop_table.set_defaults(func=do_drop_table)

# VACUUM
parser_vacuum = subparsers.add_parser('vacuum', help='Reclaim unused/wasted database space')
subparsers2 = parser_vacuum.add_subparsers()

# VACUUM TABLE
parser_vacuum_table = subparsers2.add_parser('table', help='Reclaim space by deleting failed snapshots')
parser_vacuum_table.add_argument('table', type=str, help='Zero or more tables to vacuum. If left unspecified, all tables will be vacuumed', nargs='*')
parser_vacuum_table.add_argument('-n', '--dry-run', help="Don't actually vacuum, just show what would have been vacuumed", default=False, action='store_true')
parser_vacuum_table.set_defaults(func=do_vacuum_table)


# REMOTE
parser_remote = subparsers.add_parser('remote', help='Administer remote database access')
subparsers2 = parser_remote.add_subparsers()

# REMOTE PUBLISH
parser_publish = subparsers2.add_parser('publish', help='Publish a database object (a table or a join), making it remotely accessible')
subparsers3 = parser_publish.add_subparsers()

# REMOTE PUBLISH TABLE
parser_publish_table = subparsers3.add_parser('table', help='Publish table(s)')
parser_publish_table.add_argument('table', type=str, help='Name of the table', nargs='*')
parser_publish_table.add_argument('-u', '--update', help="Update the snapshot information for all already published tables", action='store_true')
parser_publish_table.set_defaults(func=do_remote_publish_table)

# REMOTE PUBLISH JOIN
parser_publish_join = subparsers3.add_parser('join', help='Publish join definition(s)')
parser_publish_join.add_argument('join', type=str, help='Name of the join definition, the filename, or a pattern', nargs='+')
parser_publish_join.set_defaults(func=do_remote_publish_join)

# REMOTE UNPUBLISH
parser_unpublish = subparsers2.add_parser('unpublish', help='Unpublish a database object (table or join)')
subparsers3 = parser_unpublish.add_subparsers()

# REMOTE UNPUBLISH TABLE
parser_unpublish_table = subparsers3.add_parser('table', help='Unpublish table(s)')
parser_unpublish_table.add_argument('table', type=str, help='Name of the table to unpublish', nargs='+')
parser_unpublish_table.set_defaults(func=do_remote_unpublish_table)

# REMOTE UNPUBLISH JOIN
parser_unpublish_join = subparsers3.add_parser('join', help='Publish join definition(s)')
parser_unpublish_join.add_argument('join', type=str, help='Name of the join definition, the filename, or a pattern', nargs='+')
parser_unpublish_join.set_defaults(func=do_remote_unpublish_join)

# REMOTE LIST
parser_list = subparsers2.add_parser('list', help='List the objects published in the remote database')
parser_list.add_argument('remote', type=str, help='Remote database URL')
parser_list.set_defaults(func=do_remote_list)

# REMOTE LINK
parser_follow = subparsers2.add_parser('follow', help='Follow (track) remote database objects (tables)')
subparsers3 = parser_follow.add_subparsers()

# REMOTE LINK TABLE
parser_follow_table = subparsers3.add_parser('table', help='Set up a local table to follow a remote one')
parser_follow_table.add_argument('remote', type=str, help='Remote database URL')
parser_follow_table.add_argument('table', type=str, help='Name of the table', nargs='+')
parser_follow_table.add_argument('--drop-existing', help='Drop the local table if it already exists', default=False, action='store_true')
parser_follow_table.add_argument('--force', help='Set up a link even if the local table already exists', default=False, action='store_true')
parser_follow_table.set_defaults(func=do_remote_follow_table)

# REMOTE FETCH
parser_fetch = subparsers2.add_parser('fetch', help='Fetch (download) remote tables and joins')
subparsers3 = parser_fetch.add_subparsers()

# REMOTE FETCH TABLE
parser_fetch_table = subparsers3.add_parser('table', help='Fetch the contents of remote table(s)')
parser_fetch_table.add_argument('table', type=str, help='One or more tables whose contents to fetch', nargs='+')
parser_fetch_table.set_defaults(func=do_remote_fetch_table)

# REMOTE FETCH JOIN
parser_fetch_join = subparsers3.add_parser('join', help='Fetch a remote join definition')
parser_fetch_join.add_argument('remote', type=str, help='Remote database URL')
parser_fetch_join.add_argument('pattern', type=str, help='Pattern of .join files to fetch. Will be interpreted as .<pattern>.join', default='*', nargs='?')
parser_fetch_join.set_defaults(func=do_remote_fetch_join)

# REMOTE UNLINK
parser_unfollow = subparsers2.add_parser('unfollow', help='Stop following remote database objects (tables)')
subparsers3 = parser_unfollow.add_subparsers()

# REMOTE UNLINK TABLE
parser_unfollow_table = subparsers3.add_parser('table', help='Stop following a remote table')
parser_unfollow_table.add_argument('table', type=str, help='The table(s) to unlink', nargs='+')
parser_unfollow_table.set_defaults(func=do_remote_unfollow_table)


try:
	args = parser.parse_args()
	args.func(args)
except InvalidArgumentError as err:
	print err
